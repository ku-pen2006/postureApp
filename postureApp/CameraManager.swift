import AVFoundation
import Vision
import AppKit

class CameraManager: NSObject, ObservableObject {
    let session = AVCaptureSession()
    private var shoulderAngles: [Double] = []
    private var history: PostureHistory

    private var lastProcessTime: Date = Date()
    private let processInterval: TimeInterval = 1

    // ğŸ‘‡ æ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã®å‡¦ç†ç”¨
    private var lastDetectedTime: Date = Date()
    private let detectionTimeout: TimeInterval = 3 // 3ç§’ä»¥ä¸Šæ¤œå‡ºãªã—ã§ãƒªã‚»ãƒƒãƒˆ

    init(history: PostureHistory) {
        self.history = history
        super.init()
        checkCameraPermission { granted in
            if granted {
                self.setupCamera()
            } else {
                print("âŒ ã‚«ãƒ¡ãƒ©ã®åˆ©ç”¨ãŒæ‹’å¦ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒè¨­å®šã§è¨±å¯ã—ã¦ãã ã•ã„ã€‚")
            }
        }
    }

    private func checkCameraPermission(completion: @escaping (Bool) -> Void) {
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized:
            completion(true)
        case .notDetermined:
            AVCaptureDevice.requestAccess(for: .video) { granted in
                DispatchQueue.main.async { completion(granted) }
            }
        case .denied, .restricted:
            completion(false)
        @unknown default:
            completion(false)
        }
    }

    private func setupCamera() {
        session.beginConfiguration()

        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                   for: .video,
                                                   position: .unspecified),
              let input = try? AVCaptureDeviceInput(device: camera),
              session.canAddInput(input) else {
            print("âŒ ã‚«ãƒ¡ãƒ©ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        }
        session.addInput(input)

        let output = AVCaptureVideoDataOutput()
        output.setSampleBufferDelegate(self, queue: DispatchQueue(label: "camera.queue"))

        guard session.canAddOutput(output) else {
            print("âŒ ãƒ“ãƒ‡ã‚ªå‡ºåŠ›ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        }
        session.addOutput(output)

        session.commitConfiguration()
        DispatchQueue.global(qos: .userInitiated).async {
            self.session.startRunning()
        }
    }
}

extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {

        let now = Date()
        guard now.timeIntervalSince(lastProcessTime) >= processInterval else { return }
        lastProcessTime = now

        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }

        let faceRequest = VNDetectFaceLandmarksRequest()
        let bodyRequest = VNDetectHumanBodyPoseRequest()
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])

        do {
            try handler.perform([faceRequest, bodyRequest])
        } catch {
            print("âŒ Visionãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: \(error)")
            return
        }

        var detected = false

        if let faces = faceRequest.results, let face = faces.first {
            analyzeFace(face)
            detected = true
        }

        if let bodies = bodyRequest.results, let body = bodies.first {
            analyzeShoulders(body)
            detected = true
        }

        if detected {
            lastDetectedTime = now
        } else {
            // æ¤œå‡ºã§ããªã‹ã£ãŸæ™‚é–“ãŒã—ãã„å€¤ã‚’è¶…ãˆãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
            if now.timeIntervalSince(lastDetectedTime) > detectionTimeout {
                print("ğŸ™†â€â™‚ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ ã£ã¦ã„ã¾ã›ã‚“ â†’ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒªã‚»ãƒƒãƒˆ")
                history.resetSession()
            }
        }
    }

    private func analyzeFace(_ face: VNFaceObservation) {
        var status: PostureType = .good

        if let roll = face.roll?.doubleValue {
            let rollAngle = roll * 180 / .pi
            if abs(rollAngle) >= 6.0 {
                print("âš ï¸ é¡”ãŒå‚¾ã„ã¦ã„ã¾ã™")
                status = .faceTilt
            }
        }

        let faceHeight = face.boundingBox.height
        if faceHeight > 0.45 {
            print("âš ï¸ å‰å‚¾ã—ã¦ã„ã¾ã™")
            status = .forwardLean
        }

        let offset = abs(face.boundingBox.midX - 0.5)
        if offset > 0.15 {
            print("âš ï¸ æ¨ªã‚ºãƒ¬ã—ã¦ã„ã¾ã™")
            status = .sideLean
        }

        history.add(status)
    }

    private func analyzeShoulders(_ body: VNHumanBodyPoseObservation) {
        guard let points = try? body.recognizedPoints(.all),
              let leftShoulder = points[.leftShoulder], leftShoulder.confidence > 0.3,
              let rightShoulder = points[.rightShoulder], rightShoulder.confidence > 0.3 else {
            return
        }

        let dx = rightShoulder.location.x - leftShoulder.location.x
        let dy = rightShoulder.location.y - leftShoulder.location.y
        let angle = atan2(dy, dx)

        shoulderAngles.append(angle)
        if shoulderAngles.count > 10 {
            shoulderAngles.removeFirst()
        }
        let avgAngle = shoulderAngles.reduce(0,+) / Double(shoulderAngles.count)
        let avgAngleDegrees = avgAngle * 180 / .pi

        if abs(avgAngleDegrees) >= 5.0 {
            print("âš ï¸ è‚©ãŒå‚¾ã„ã¦ã„ã¾ã™")
            history.add(.shoulderTilt)
        } else {
            history.add(.good)
        }
    }
}
