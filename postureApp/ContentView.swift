import SwiftUI
import AVFoundation
import Vision
import AppKit

// MARK: - ã‚«ãƒ¡ãƒ©ç®¡ç†
class CameraManager: NSObject, ObservableObject {
    let session = AVCaptureSession()
    private var shoulderAngles: [Double] = [] // è‚©è§’åº¦ã®å±¥æ­´

    override init() {
        super.init()
        setupCamera()
    }

    private func setupCamera() {
        session.beginConfiguration()

        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                   for: .video,
                                                   position: .unspecified), // .front ã‚ˆã‚Š .unspecified ã®æ–¹ãŒMacã§ã¯ç¢ºå®Ÿ
              let input = try? AVCaptureDeviceInput(device: camera),
              session.canAddInput(input) else {
            print("âŒ ã‚«ãƒ¡ãƒ©ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        }
        session.addInput(input)

        let output = AVCaptureVideoDataOutput()
        output.setSampleBufferDelegate(self, queue: DispatchQueue(label: "camera.queue"))

        guard session.canAddOutput(output) else {
            print("âŒ ãƒ“ãƒ‡ã‚ªå‡ºåŠ›ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        }
        session.addOutput(output)

        session.commitConfiguration()

        DispatchQueue.global(qos: .userInitiated).async {
            self.session.startRunning()
        }
    }
}

// MARK: - å§¿å‹¢åˆ¤å®šå‡¦ç†
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput,
                       didOutput sampleBuffer: CMSampleBuffer,
                       from connection: AVCaptureConnection) {

        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }

        let faceRequest = VNDetectFaceLandmarksRequest()
        let bodyRequest = VNDetectHumanBodyPoseRequest()

        // ğŸ’¡ å‘ã(orientation)ã®æŒ‡å®šã‚’å‰Šé™¤
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        
        do {
            try handler.perform([faceRequest, bodyRequest])
        } catch {
            print("âŒ Visionãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œã«å¤±æ•—: \(error)")
            return
        }

        // --- é¡”åˆ¤å®š ---
        // ğŸ’¡ as? ã§ã®ã‚­ãƒ£ã‚¹ãƒˆãŒä¸è¦ãªãŸã‚ä¿®æ­£
        if let faces = faceRequest.results, let face = faces.first {
            analyzeFace(face)
        }

        // --- è‚©åˆ¤å®š ---
        // ğŸ’¡ as? ã§ã®ã‚­ãƒ£ã‚¹ãƒˆãŒä¸è¦ãªãŸã‚ä¿®æ­£
        if let bodies = bodyRequest.results, let body = bodies.first {
            analyzeShoulders(body)
        }
    }

    private func analyzeFace(_ face: VNFaceObservation) {
        if let roll = face.roll?.doubleValue {
            // ãƒ©ã‚¸ã‚¢ãƒ³ã‹ã‚‰åº¦ã¸ã®å¤‰æ›ï¼ˆåˆ†ã‹ã‚Šã‚„ã™ã•ã®ãŸã‚ï¼‰
            let rollAngle = roll * 180 / .pi
            if abs(rollAngle) < 6.0 { // Â±6åº¦ä»¥å†…
                print("ğŸ™‚ é¡”ã¯æ°´å¹³")
            } else {
                print("âš ï¸ é¡”ãŒå‚¾ã„ã¦ã„ã¾ã™ (roll=\(String(format: "%.1f", rollAngle))åº¦)")
            }
        }

        // å‰å‚¾ãƒã‚§ãƒƒã‚¯ï¼ˆé¡”ã®å¤§ãã•ã§åˆ¤å®šï¼‰
        let faceHeight = face.boundingBox.height
        if faceHeight > 0.45 {
            print("âš ï¸ å‰å‚¾ã—ã¦ã„ã¾ã™ï¼ˆç”»é¢ã«è¿‘ã™ãï¼‰")
        }

        // æ¨ªã‚ºãƒ¬ãƒã‚§ãƒƒã‚¯
        let offset = abs(face.boundingBox.midX - 0.5)
        if offset > 0.15 { // é–¾å€¤ã‚’å°‘ã—åºƒã’ã‚‹
            print("âš ï¸ æ¨ªã«ã‚ºãƒ¬ã¦ã„ã¾ã™")
        }
    }

    private func analyzeShoulders(_ body: VNHumanBodyPoseObservation) {
        // ğŸ’¡ recognizedPointsã®æŒ‡å®šæ–¹æ³•ã‚’ä¿®æ­£
        guard let points = try? body.recognizedPoints(.all),
              let leftShoulder = points[.leftShoulder], leftShoulder.confidence > 0.3, // èªè­˜ä¿¡é ¼åº¦ã®é–¾å€¤ã‚’å°‘ã—ä¸‹ã’ã‚‹
              let rightShoulder = points[.rightShoulder], rightShoulder.confidence > 0.3 else {
            return
        }

        let dx = rightShoulder.location.x - leftShoulder.location.x
        let dy = rightShoulder.location.y - leftShoulder.location.y
        let angle = atan2(dy, dx)

        // å¹³æ»‘åŒ–ï¼ˆç›´è¿‘10ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡ï¼‰
        shoulderAngles.append(angle)
        if shoulderAngles.count > 10 {
            shoulderAngles.removeFirst()
        }
        let avgAngle = shoulderAngles.reduce(0,+) / Double(shoulderAngles.count)
        
        // ãƒ©ã‚¸ã‚¢ãƒ³ã‹ã‚‰åº¦ã¸ã®å¤‰æ›
        let avgAngleDegrees = avgAngle * 180 / .pi

        if abs(avgAngleDegrees) < 5.0 { // Â±5åº¦ä»¥å†…
            print("âœ… è‚©ã¯æ°´å¹³")
        } else {
            print("âš ï¸ è‚©ãŒå‚¾ã„ã¦ã„ã¾ã™ (avg=\(String(format: "%.1f", avgAngleDegrees))åº¦)")
        }
    }
}

// MARK: - SwiftUIãƒ“ãƒ¥ãƒ¼
struct ContentView: View {
    @StateObject private var cameraManager = CameraManager()

    var body: some View {
        CameraPreview(session: cameraManager.session)
            .ignoresSafeArea()
    }
}

// MARK: - ã‚«ãƒ¡ãƒ©ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
struct CameraPreview: NSViewRepresentable {
    let session: AVCaptureSession

    func makeNSView(context: Context) -> NSView {
        let view = NSView()
        view.wantsLayer = true

        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
        previewLayer.videoGravity = .resizeAspectFill
        
        // ğŸ’¡ viewã®layerãŒç¢ºå®šã—ã¦ã‹ã‚‰frameã‚’è¨­å®š
        view.layer = CALayer()
        view.layer?.addSublayer(previewLayer)
        
        // ğŸ’¡ AutoresizingMaskã‚’è¨­å®šã—ã¦ãƒªã‚µã‚¤ã‚ºã«å¯¾å¿œ
        previewLayer.autoresizingMask = [.layerWidthSizable, .layerHeightSizable]
        previewLayer.frame = view.bounds

        return view
    }

    func updateNSView(_ nsView: NSView, context: Context) {}
}
//import SwiftUI
//import AVFoundation
//import Vision
//import AppKit  // Macã‚¢ãƒ—ãƒªç”¨
//
//// MARK: - ã‚«ãƒ¡ãƒ©ç®¡ç† & å§¿å‹¢æ¤œçŸ¥
//class CameraManager: NSObject, ObservableObject {
//    let session = AVCaptureSession()
//    
//    override init() {
//        super.init()
//        setupCamera()
//    }
//    
//    private func setupCamera() {
//        session.beginConfiguration()
//        
//        // Macã®ã‚«ãƒ¡ãƒ©ã‚’å–å¾—
//        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera,
//                                                   for: .video,
//                                                   position: .front),
//              let input = try? AVCaptureDeviceInput(device: camera),
//              session.canAddInput(input) else {
//            print("ã‚«ãƒ¡ãƒ©ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
//            return
//        }
//        session.addInput(input)
//        
//        // ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®å‡ºåŠ›ã‚’è¨­å®š
//        let output = AVCaptureVideoDataOutput()
//        output.setSampleBufferDelegate(self, queue: DispatchQueue(label: "camera.queue"))
//        
//        guard session.canAddOutput(output) else {
//            print("ãƒ“ãƒ‡ã‚ªå‡ºåŠ›ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
//            return
//        }
//        session.addOutput(output)
//        
//        session.commitConfiguration()
//        
//        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
//        DispatchQueue.global(qos: .userInitiated).async {
//            self.session.startRunning()
//        }
//    }
//}
//
//// å§¿å‹¢æ¤œçŸ¥ãƒ­ã‚¸ãƒƒã‚¯
//extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
//    func captureOutput(_ output: AVCaptureOutput,
//                       didOutput sampleBuffer: CMSampleBuffer,
//                       from connection: AVCaptureConnection) {
//        
//        // â‘  ã¾ãšã€ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒå‘¼ã°ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
//        print("--- ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¾ã—ãŸ ---")
//        
//        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
//        
//        let request = VNDetectHumanBodyPoseRequest()
//        
//        do {
//            // âœ… ã“ã“ã§ orientation ã‚’ .leftMirrored ã«å¤‰æ›´
//                let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer,
//                                                    orientation: .leftMirrored, // â† ã“ã“ãƒã‚¤ãƒ³ãƒˆ
//                                                    options: [:])
//            try handler.perform([request])
//        } catch {
//            print("âŒ Visionãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: \(error)")
//            return
//        }
//        
//        print("ğŸ” æ¤œå‡ºçµæœ:", request.results?.count ?? 0)
//        // â‘¡ VisionãŒã€Œäººã€ã‚’æ¤œå‡ºã—ãŸã‹ã‚’ç¢ºèª
//        guard let results = request.results, !results.isEmpty else {
//            print("ğŸ‘¤ äººãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
//            return
//        }
//        print("âœ… \(results.count)äººã®äººã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
//        
//        for observation in results {
//            guard let points = try? observation.recognizedPoints(.all) else {
//                print("âš ï¸ é–¢ç¯€ãƒã‚¤ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
//                continue
//            }
//            
//            // â‘¢ å¿…è¦ãªé–¢ç¯€ã®ä¿¡é ¼åº¦ï¼ˆConfidenceï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
//            let lsConf = points[.leftShoulder]?.confidence ?? 0
//            let rsConf = points[.rightShoulder]?.confidence ?? 0
//            let noseConf = points[.nose]?.confidence ?? 0
//            let leConf = points[.leftEar]?.confidence ?? 0
//            let reConf = points[.rightEar]?.confidence ?? 0
//            
//            print(String(format: "ä¿¡é ¼åº¦ -> è‚©(L:%.2f, R:%.2f), é¼»:%.2f, è€³(L:%.2f, R:%.2f)", lsConf, rsConf, noseConf, leConf, reConf))
//            
//            // â‘£ ä¿¡é ¼åº¦ãŒ0.5ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
//            guard lsConf > 0.5, rsConf > 0.5, noseConf > 0.5, leConf > 0.5, reConf > 0.5 else {
//                print("ğŸ“‰ ä¿¡é ¼åº¦ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€å§¿å‹¢åˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
//                continue
//            }
//            
//            // --- ã“ã“ã¾ã§åˆ°é”ã™ã‚Œã°ã€æœ€çµ‚çš„ãªå§¿å‹¢åˆ¤å®šãŒè¡Œã‚ã‚Œã‚‹ã¯ãš ---
//            print("ğŸ‘ ä¿¡é ¼åº¦ã®æ¡ä»¶ã‚’ã‚¯ãƒªã‚¢ï¼å§¿å‹¢ã‚’åˆ¤å®šã—ã¾ã™ã€‚")
//            
//            // (å…ƒã®åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã¯çœç•¥)
//            let shoulderWidth = abs(points[.leftShoulder]!.location.x - points[.rightShoulder]!.location.x)
//            let shoulderCenter = CGPoint(x: (points[.leftShoulder]!.location.x + points[.rightShoulder]!.location.x) / 2, y: (points[.leftShoulder]!.location.y + points[.rightShoulder]!.location.y) / 2)
//            let horizontalOffset = abs(points[.nose]!.location.x - shoulderCenter.x)
//            let isCentered = horizontalOffset < (shoulderWidth * 0.15)
//            let verticalTilt = abs(points[.leftEar]!.location.y - points[.rightEar]!.location.y)
//            let isLevel = verticalTilt < (shoulderWidth * 0.10)
//            
//            if isCentered && isLevel {
//                print("âœ… è‰¯ã„å§¿å‹¢ã§ã™ï¼")
//            } else {
//                print("âš ï¸ å§¿å‹¢ãŒå´©ã‚Œã¦ã„ã¾ã™")
//            }
//        }
//    }
//}
//
//// MARK: - SwiftUIãƒ“ãƒ¥ãƒ¼
//struct ContentView: View {
//    @StateObject private var cameraManager = CameraManager()
//    
//    var body: some View {
//        CameraPreview(session: cameraManager.session)
//            .ignoresSafeArea()
//    }
//}
//
//// MARK: - Macç”¨ã®ã‚«ãƒ¡ãƒ©ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
//struct CameraPreview: NSViewRepresentable {
//    let session: AVCaptureSession
//    
//    func makeNSView(context: Context) -> NSView {
//        let view = NSView()
//        view.wantsLayer = true // NSViewãŒãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’æŒã¤ã‚ˆã†ã«è¨­å®š
//        
//        let previewLayer = AVCaptureVideoPreviewLayer(session: session)
//        previewLayer.videoGravity = .resizeAspectFill
//        
//        // viewã®ã‚µã‚¤ã‚ºå¤‰æ›´ã«previewLayerãŒè¿½å¾“ã™ã‚‹ã‚ˆã†ã«è¨­å®š
//        previewLayer.frame = view.bounds
//        previewLayer.autoresizingMask = [.layerWidthSizable, .layerHeightSizable]
//        
//        view.layer?.addSublayer(previewLayer)
//        
//        return view
//    }
//    
//    func updateNSView(_ nsView: NSView, context: Context) {}
//}
